
  "readme_md_content": "# AI Voice Sales Agent\n\n[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) <!-- Choose your license -->\n\nThis project implements an AI-powered Voice Sales Agent capable of making simulated automated phone calls to potential customers to pitch online courses. It features natural conversation capabilities, objection handling, and a FastAPI backend, primarily utilizing CPU for AI model inference.\n\n**Live Demo Video:** [Link to your 3-5 minute demo video - if applicable for submission]\n\n## Table of Contents\n\n- [Core Features](#core-features)\n- [Project Structure](#project-structure)\n- [Technical Stack](#technical-stack)\n- [Setup Instructions](#setup-instructions)\n  - [1. Prerequisites](#1-prerequisites)\n  - [2. Clone the Repository](#2-clone-the-repository)\n  - [3. Set Up Python Virtual Environment](#3-set-up-python-virtual-environment)\n  - [4. Install Dependencies](#4-install-dependencies)\n  - [5. Configure Environment Variables](#5-configure-environment-variables)\n  - [6. Model Downloads (Automatic on First Run)](#6-model-downloads-automatic-on-first-run)\n- [Running the Application](#running-the-application)\n  - [1. Start the FastAPI Server](#1-start-the-fastapi-server)\n  - [2. Run a Client Application](#2-run-a-client-application)\n- [Running Tests](#running-tests)\n- [Performance Notes](#performance-notes)\n- [Troubleshooting](#troubleshooting)\n- [Future Enhancements](#future-enhancements)\n- [Contributing](#contributing)\n- [License](#license)\n\n## Core Features\n\n*   **Voice Conversation System:**\n    *   Text-to-Speech (TTS) capability using `pyttsx3`.\n    *   Speech-to-Text (STT) capability using OpenAI Whisper (via the `whisper` library).\n    *   Real-time conversation flow management via WebSockets.\n*   **Sales Intelligence:**\n    *   Dynamic conversation scripts driven by a local Qwen2-0.5B-Instruct LLM (via Hugging Face `transformers` library on CPU).\n    *   Objection handling and lead qualification logic embedded within the LLM's prompting.\n*   **Backend API:**\n    *   FastAPI server providing HTTP and WebSocket endpoints.\n    *   Conversation state management.\n*   **LangChain Integration:**\n    *   Uses `langchain-core` for message structuring (`HumanMessage`, `AIMessage`).\n    *   Employs `ChatPromptTemplate` and LangChain Expression Language (LCEL) for managing conversation flow with the LLM.\n    *   Custom LangChain `LLM` wrapper for the local Qwen model loaded via `transformers`.\n\n## Project Structure\n\n```\nAI_VOICE_CLIENT-MAIN/\n├── app/\n│   ├── api/\n│   │   └── v1/\n│   │       └── endpoints/\n│   │           └── call_router.py\n│   ├── core/\n│   │   ├── config.py\n│   │   ├── conversation_manager.py\n│   │   └── rag_setup.py (Present if RAG was added, can be omitted if not)\n│   ├── models/ (Pydantic models for API)\n│   │   └── call_models.py\n│   ├── prompts/\n│   │   └── sales_prompts.py\n│   ├── schemas/ (Pydantic schemas for data structures)\n│   │   └── conversation.py\n│   └── services/\n│       ├── init.py # As listed in your structure\n│       ├── llm_service.py # Uses Hugging Face Transformers\n│       ├── stt_service.py\n│       └── tts_service.py\n├── .huggingface_cache_project/ (Local cache for Hugging Face models)\n│   └── ... # Contents of cache, gitignored\n├── test_scripts/ # Folder for test scripts\n│   ├── init.py # As listed in your structure\n│   ├── test_config.py\n│   ├── test_llm_service.py\n│   ├── test_stt_service.py\n│   ├── test_tts_service.py\n│   ├── test_conversation_manager_with_real_llm.py\n│   └── test_http_endpoints.py\n├── venv/ (Python virtual environment)\n│   └── ... # Contents of virtual environment, gitignored\n├── .env\n├── .env.example\n├── .gitignore\n├── interactive_voice_chat.py (CLI for local voice interaction)\n├── websocket_voice_client.py (CLI for WebSocket based voice interaction)\n├── main.py (FastAPI application entry point)\n├── requirements.txt\n└── README.md\n```\n\n## Technical Stack\n\n*   **Language:** Python 3.10+\n*   **Backend Framework:** FastAPI\n*   **AI Framework:** LangChain\n*   **LLM:** Qwen2-0.5B-Instruct (via Hugging Face `transformers` library, running on CPU)\n*   **STT:** OpenAI Whisper (via `whisper` library)\n*   **TTS:** `pyttsx3` (utilizing OS-level speech engines)\n*   **WebSocket Communication:** `websockets` library (used by FastAPI)\n*   **Audio Handling:** `sounddevice`, `soundfile`, `numpy`\n*   **Model Loading & Management:** Hugging Face `transformers`, `accelerate` (for `device_map=\"cpu\"`)\n\n## Setup Instructions\n\n### 1. Prerequisites\n\n*   **Python:** Version 3.10 or newer.\n*   **Pip:** Python package installer.\n*   **Git:** For cloning the repository.\n*   **Microphone and Speakers/Headphones:** For voice interaction.\n*   **`ffmpeg`:** Required by the `whisper` library for audio processing.\n    *   **Windows:** Download from [ffmpeg.org](https://ffmpeg.org/download.html), extract, and add the `bin` directory to your system's PATH.\n    *   **Linux:** `sudo apt-get install ffmpeg` (or `yum install ffmpeg`).\n    *   **macOS:** `brew install ffmpeg`.\n\n### 2. Clone the Repository\n\n```bash\ngit clone [URL_OF_YOUR_REPOSITORY] # Replace [URL_OF_YOUR_REPOSITORY] with your actual repo URL\ncd AI_VOICE_CLIENT-MAIN\n```\n\n### 3. Set Up Python Virtual Environment\n\nIt's highly recommended to use a virtual environment to manage project dependencies.\n\n```bash\n# Create a virtual environment (e.g., named 'venv')\npython -m venv venv\n\n# Activate the virtual environment\n# Windows:\nvenv\\Scripts\\activate\n# macOS/Linux:\nsource venv/bin/activate\n```\n\n### 4. Install Dependencies\n\nCreate a `requirements.txt` file in your project root with the following content (if it doesn't exist already or needs updating):\n\n```text\n# requirements.txt\nfastapi\nuvicorn[standard]\npython-dotenv\npydantic-settings\nloguru\ntorch\ntransformers\naccelerate\n# For LangChain core and custom LLM wrapper\nlangchain-core\n# For STT (OpenAI Whisper)\nopenai-whisper\n# For TTS\npyttsx3\n# For audio I/O and processing\nsounddevice\nsoundfile\nnumpy\n# For WebSocket client (if testing separately) and FastAPI WebSockets\nwebsockets\n# For HTTP client in tests\nrequests\n# Optional, but good for LangChain integrations (if you expand later)\n# langchain-community\n```\n\nThen, install the packages:\n\n```bash\npip install -r requirements.txt\n```\n\n### 5. Configure Environment Variables\n\n1.  **Create `.env.example` (Template for Environment Variables):**\n    Ensure a file named `.env.example` exists in the project root directory (`AI_VOICE_CLIENT-MAIN/`). This file serves as a template for your actual configuration. If it's missing or needs updating, use the following content:\n\n    ```dotenv\n    # .env.example\n\n    # App Configuration\n    APP_TITLE=\"AI Voice Sales Agent (Qwen LLM)\"\n    APP_VERSION=\"0.3.0\"\n    LOG_LEVEL=\"INFO\" # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL\n\n    # LLM Configuration (Hugging Face Transformers based)\n    # For Qwen models, a token is not strictly required for download if public,\n    # but good practice for accessing gated models or increasing rate limits.\n    # Get from https://huggingface.co/settings/tokens\n    HUGGINGFACE_API_TOKEN=\"your_huggingface_api_token_here_if_needed\"\n    LLM_MODEL_REPO_ID=\"Qwen/Qwen2-0.5B-Instruct\"\n    # This directs Hugging Face libraries to use a local cache directory.\n    # The path should be relative to the project root or an absolute path.\n    HF_HOME=\"./.huggingface_cache_project\"\n\n    # STT Configuration (OpenAI Whisper model size)\n    # Options: \"tiny\", \"base\", \"small\", \"medium\", \"large\", \"tiny.en\", \"base.en\", etc.\n    WHISPER_MODEL_SIZE=\"base\"\n\n    # Course Information (used in prompts for the sales agent)\n    COURSE_NAME=\"AI Mastery Bootcamp\"\n    COURSE_DURATION=\"12 weeks\"\n    COURSE_PRICE_FULL=\"$499\"\n    COURSE_PRICE_SPECIAL=\"$299\"\n    COURSE_BENEFITS_LIST='[\"Learn LLMs, Computer Vision, and MLOps\", \"Hands-on projects\", \"Job placement assistance\", \"Certificate upon completion\"]'\n    ```\n\n2.  **Create your `.env` file:**\n    Copy the `.env.example` to create your actual environment file:\n\n    ```bash\n    cp .env.example .env\n    ```\n\n3.  **Adjust `.env` values:**\n    Open the newly created `.env` file and adjust any values if necessary. The defaults provided in `.env.example` should work for a basic run, but you might want to add your `HUGGINGFACE_API_TOKEN` or change model preferences.\n    **Important:** The `.env` file contains sensitive information or local configurations and should **not** be committed to version control. Ensure it's listed in your `.gitignore` file.\n\n### 6. Model Downloads (Automatic on First Run)\n\nThe first time you run the application or a test script that initializes the LLM or STT services, the necessary models (e.g., Qwen/Qwen2-0.5B-Instruct from Hugging Face and the Whisper model) will be downloaded.\n\n*   These models will be cached locally in the directory specified by the `HF_HOME` environment variable (default is `.huggingface_cache_project/` in the project root, as per the `.env.example`).\n*   This process may take some time and requires an active internet connection.\n\n## Running the Application\n\n### 1. Start the FastAPI Server\n\nThe server hosts the API endpoints and WebSocket for communication.\n\n```bash\n# Ensure your virtual environment is activated\nuvicorn app.main:app --host 0.0.0.0 --port 8000 --reload\n```\n\n*   `--reload` is for development; it automatically restarts the server on code changes. Remove this flag for production.\n*   The server will be accessible at `http://localhost:8000`.\n*   API documentation (Swagger UI) will be available at `http://localhost:8000/docs`.\n*   A health check endpoint is available at `http://localhost:8000/health`.\n*   **Observe Server Logs:** Upon startup, check the console for logs indicating successful initialization of LLM, STT, and TTS services. It should confirm the LLM is running on CPU (unless configured otherwise).\n\n### 2. Run a Client Application\n\nYou have two command-line clients for interacting with the agent:\n\n#### A. Interactive Voice Chat (Local, Direct Service Test)\n\nThis script tests the core STT, LLM, and TTS services locally, bypassing the FastAPI server. It's useful for quick checks of the individual AI components.\n\n```bash\n# Open a new terminal, ensure your virtual environment is activated\npython interactive_voice_chat.py\n```\n\nFollow the prompts to enter your name and interact using your voice.\n\n#### B. WebSocket Voice Client (Full End-to-End Test)\n\nThis client connects to the running FastAPI server via WebSockets for a complete voice chat experience, testing the entire system.\n\n```bash\n# Ensure the FastAPI server is running in another terminal\n# Open a new terminal, ensure your virtual environment is activated\npython websocket_voice_client.py\n```\n\nFollow the prompts to enter your name. Press `Enter` to start speaking, and `Enter` again to stop and send your audio.\n\n## Running Tests\n\nThe project includes several test scripts in the `test_scripts/` directory to verify individual components. Run these from the project root with your virtual environment activated.\n\n**To check configuration loading:**\n```bash\npython test_scripts/test_config.py\n```\n\n**To test Speech-to-Text (may include live recording):**\n```bash\npython test_scripts/test_stt_service.py\n```\n\n**To test Text-to-Speech:**\n```bash\npython test_scripts/test_tts_service.py\n```\n\n**To test the LLM service (using local transformers on CPU):**\n```bash\npython test_scripts/test_llm_service.py\n```\n\n**To test the conversation manager with the actual LLM and LCEL chain:**\n```bash\npython test_scripts/test_conversation_manager_with_real_llm.py\n```\n\n**To test HTTP endpoints (ensure the FastAPI server is running first):**\n```bash\n# Terminal 1: Start server (if not already running)\n# uvicorn app.main:app --host 0.0.0.0 --port 8000\n\n# Terminal 2: Run HTTP tests\npython test_scripts/test_http_endpoints.py\n```\n\n## Performance Notes\n\n*   **CPU Bound:** This project is configured to run AI models (LLM, STT) on the CPU using the Hugging Face `transformers` library. As a result, there will be noticeable latency (e.g., 4-5 seconds or more per LLM response) during conversation turns.\n*   **Optimization:** For significantly lower latency, GPU acceleration or specialized CPU runtimes (like Llama.cpp for some models) would be required. This setup prioritizes broader compatibility and easier initial setup by focusing on CPU execution with standard libraries.\n\n## Troubleshooting\n\n*   **`ffmpeg` not found:** Ensure `ffmpeg` is installed and accessible in your system's PATH. This is crucial for the `whisper` library.\n*   **Model download issues:** Check your internet connection. If downloads are interrupted, you might need to clear the partial contents from your `HF_HOME` directory (e.g., `.huggingface_cache_project/`) and try again.\n*   **Microphone/Speaker issues:** Ensure your operating system has the correct audio input/output devices selected and that the Python scripts have the necessary permissions to access them.\n*   **Import Errors:** Double-check that all packages listed in `requirements.txt` are correctly installed in your active virtual environment (you can verify with `pip list`).\n*   **Slow Performance:** This is expected with the current configuration running LLMs on CPU. Refer to \"Performance Notes.\"\n\n## Future Enhancements\n\n*   GPU acceleration for LLM and STT to significantly reduce latency.\n*   Integration of faster local TTS engines (e.g., Piper TTS, Coqui TTS).\n*   Implementation of Retrieval Augmented Generation (RAG) for a more extensive and dynamic knowledge base.\n*   Full end-to-end audio streaming for improved perceived real-time interaction (reduces start-stop feeling).\n*   A simple web interface (e.g., using Streamlit or Gradio) for easier testing and demonstration.\n\n## Contributing\n\nContributions are welcome! If you'd like to contribute, please follow these steps:\n\n1.  Fork the repository.\n2.  Create a new branch (`git checkout -b feature/your-feature-name`).\n3.  Make your changes and commit them (`git commit -m 'Add some feature'`).\n4.  Push to the branch (`git push origin feature/your-feature-name`).\n5.  Open a Pull Request.\n\nPlease make sure to update tests as appropriate.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.\n(Note: You'll need to create a `LICENSE.md` file in your repository. If you choose MIT, you can find a template easily online.)\n"
